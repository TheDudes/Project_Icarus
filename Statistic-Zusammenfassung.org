Statistic-Zusammenfassung -*- mode: org -*-
#+STARTUP: showall
#+OPTIONS: tex:t
#+LaTeX_CLASS_OPTIONS: [a4paper]

* Definition oft verwendeter variablen und functionen
  + $p(X)$ = Wahrscheinlichkeit von X (bei W6, $\frac{1}{6}$)
  + $X$    = ein Ereignis (bei einem W6 z.b: 1|2|3|4|5|6)
  + \Omega = Ergebnismenge (menge von Ereignissen, bei 2xW6 $\{(1,1),(1,2),(2,2),...,(6,6)\}$)
  + $E(X)$ = Erwartungswert (beim W6 = $3.5 = p(x1)*x1+p(x2)*x2...p(xn)*xn$)
  + Zufallsvariablen = sind Ergebnisse einer Function, beim Würfeln mit zwei Würfeln\\
    beispielsweiße die augensumme des Ereignisses. (menge von Zufallsvariablen $\{2,3,4,...,12\}$)\\
    Hat die Zufallsvariable keinen Diskreten Wertebereich dann ist sie Stetig.
    
* Varianz
  + Varianz(X) bei Diskreten Zufallsvariablen ($\mu = E(X)$)\\
    $V(X) = E((X-\mu)^2) = E(X^2)-\mu^2 = E(X^2)-E(X)^2$
    
* Standardabweichung
  Standardabweichung = $\sigma = \sqrt(V(X))$
  
* Laplace-Raum
  + Skriptum Seite: 8
** eigenschaften Laplace-Raum
   + Experiment hat endlich viele Elemente
   + jedes Ergebnis hat die gleiche Wahrscheinlichkeit
     
* Urnen-Experimente
  + Ziehen ohne zurücklegen\\
    Faustregel: $n > 1500*p$ & $n * p < 10$ -> dann poisson möglich statt binomial.
    
* Bernoulli-Experimente
  + Ziehen aus Urne mit zurücklegen. Außer urne hat “sehr viele” Elemente\\
    dann ist auch ohne zurückgelgen die Vorraussetzung für ein Bernoulli-Experiment erfüllt.
     
* Diskrete verteilungen
  + der Werteberiech (Ereignismenge) der Zufallsvariablen $X$ ist abzählbar oder endlich\\
    lassen sich durch eine Wahrscheinlichkeitsfunction $p(x) = p(X=x)$
** Gleich verteilung (Seite 40)
   - $E(X) = \frac{1}{n} * \sum_{i=1}^{n} x_i$
   - $n$ = die anzahl der Elemente im Laplaceraum
   - $\Omega$ = Laplaceraum
   - $Var(X) = \sum_{i=1}^{n} x_i^2 * \frac{1}{n} - E(x)^2$ \\
     Beispiel: Würfeln mit einem Würfel\\
     $$Var(x) = \frac{1}{6} * 1^2 + \frac{1}{6} * 2^2 + ... + \frac{1}{6} * 6^2 - 3,5^5 = 2,917$$
** Binomial verteilung (Seite 41)
   - Sie beschreibt die Anzahl der Erfolge in einer Serie von gleichartigen und unabhängigen Versuchen,\\
     die jeweils genau zwei mögliche Ergebnise haben.(Bernoulliexperiment)
   - Faustregel: $n > 1500*p$ & $n * p < 10$ -> dann poisson möglich
   - $A$ = wahrscheinlichkeit eines einzelnen ereignisses, $\frac{1}{6}$ bei W6
   - $X = k$ <=> $A$ tritt $k$ mal ein
   - $p(X=k) = {n \choose k}*p(A)^k*(1-p(A))^n-k$
   - $E(X) = n*p(A)$
   - $Var(X) = n*p(A)*(1-p(A))$
   - ${n \choose k}$ := $\frac{n*(n-1)*(n-2)...(n-k+1)}{1*2*3...*k}$ \\
     Beispiel: ${5 \choose 2} = \frac{5*(5-2+1)}{1 * 2} = \frac{5 * 4}{2}$
** Hypergeometrische verteilung (Seite 44)
   - das ist die verteilung eines Urnenexperiements ohne zurücklegen.
   - $p := \frac{S}{N}$
   - $E(X) = n*p$
   - $Var(X) = n*p*(1-p)*\frac{N-n}{N-1}$ ($N$ sind alle Kugeln)
   - $p(X=k) = {S \choose k}*\frac{{N-S \choose n-k}}{{N \choose n}}$ \\
     ($S$ alle schwarzen Kugeln, $k$ anzahl der gezogenen schwarzen Kugeln)
** geometrische verteilung (Seite 45)
   - Diese gibt in einem Bernoulliexperiment an, wie lange man auf das Eintreten des Ereignisses warten\\
     muss.
   - $A$ = wahrscheinlichkeit eines einzelnen ereignisses, $\frac{1}{6}$ bei W6
   - $p(X=k) = p(A)*(1-p(A))^k-1$
   - $E(X) = \frac{1}{p(a)}$
   - $Var(X) = \frac{1-p(A)}{p(A)^2}$
** Poisson verteilung (Seite 47)
   - Beobachtung eines Seltenen Ereignisses, nicht wie oft schlägt der blitz nicht ein sondern wie oft\\
     schlägt der Blitz ein, recursive annäherung and die binomialverteilung
   - $A$ = wahrscheinlichkeit eines einzelnen ereignisses, $\frac{1}{6}$ bei W6
   - $\lambda = n*p(A)$
   - $p(X=k) = \frac{\lambda^k}{k!}*e^{-\lambda}$
   - $E(X) = \lambda$
   - $Var(X) = \lambda$
    
* Stetige Verteilungen
  + stetige verteilungen werden durch eine Stetige verteilungsfunction $F(x) = p(X<=x)$\\
    characterisiert. überabzählbar(nicht abzählbar) wertebereich(Ereignismenge) der Zufallsvariablen,\\
    wobei die wahrscheinlichkeit für jeden einzelnen wert gleich null is. 
** standard normal verteilung (Seite 59)
   - damit lässt sich die Abweichung vom Erwartungswert berechnen. Die null stellt die mitta dar und\\
     steht für den Erwartungswert, links und rechts von der null befindet sich die Abweichung.
   - $E(X) = 0$
   - $Var(X) = 1$
   - wenn $np > 5$ und $n(1-p)>5$ dann folgendes möglich:
     $$p(X \leq k)=\Phi \left( \frac{k-np+05}{\sqrt{np(1-p)}} \right)$$
     $$p(X<k)=\Phi \left( \frac{k-np-05}{\sqrt{np(1-p)}} \right)$$
** allgemein normal verteilung (Seite 61)
   - wenn mein erwartungswert richtig gewählt ist und ich viele datensätze habe (mein N groß ist)\\
     wird sich mein Ergebnis an die Normalverteilung annähern (Gauß glockenkurve).\\
     Faustregel: $n * p > 5$ und  $n(1-p) > 5$ -> dann binomial möglich
   - $p(X=x) = \Phi ( \frac{x - \mu}{\sigma} )$
** exponetial verteilung (Seite 64)
   - Warten auf die sechs beim würfeln
   - $A$ = wahrscheinlichkeit eines einzelnen ereignisses, $\frac{1}{6}$ bei W6
   - $\lambda = n*p(A)$
   - $t$ = zeit bis zum eintreten des Ereignisses
   - $E(X) = \frac{1}{\lambda}$
   - $Var(X) = \frac{1}{\lambda^2}$
   - $p(X \leq t) = 1 - e^{-\lambda t}$
   - $p(X \geq t) = e^{-\lambda t}$
** $\chi^2$ verteilung (Seite 67)
   - Wie gut sind die berechneten Zufallszahlen\\
     der $\chi^2$-test macht eine Aussage darüber, ob die beobachteten Häufigkeiten sich signifikant\\
     von denen unterscheiden die man erwarten würde.
   - $E( \chi_n^2) = n$ pro n
   - $Var( \chi_n^2) = 2n$ pro n
** stetige gleichverteilung
   - alle intervalle meiner ereignismenge gleicher länge, haben die gleiche wahrscheinlichkeit\\
     Beispiel: Uhrzeiger
   - $E(X) = \frac{a+b}{2}$

* Parameterschätzung
** Stichprobe (Seite 68)
   - \overline{x} = \frac{X_1 + X_2 + ... + X_n}{n}\\
     Für große $n$: $E(X) = \overline{x}$
   - $Var(X)^2 = \frac{1}{n-1}*\left( (x_1^2 + x_2^2 + ... + x_n^2) -n \overline{x}^2 \right)$
** Konfidenzintervalle (Seite 75)
   - Konfidenzniveau = $\gamma$
   - Konfidenzintervall = $[p_o, p_u]$
   - $c = \Sigma^-1(\frac{1+ \gamma}{2})$
   - Standartisierte $X^* = \frac{X-np}{\sqrt{np(1-p)}}$
** Hypothesentest (Seite 80)
   - $\alpha$ ist die irrtumswahrscheinlichkeit
   - $c = \Sigma^-1(1-\frac{\alpha}{2})$
   - $\delta$ gibt die maximal abweichung an\\
     $\delta = c * \sqrt{\frac{p_0(1-p_0)}{n}}$
   - Nichtablehnungsbereiche:\\
     $k/n \notin [p_0- \delta, p_0+ \delta]$\\
     $k \notin [np_0 - n\delta, np_0 + n\delta]$
** einseitiger Hypothesentest (Seite 84)
   - $c = \Sigma^-1(1-\alpha)$
   - Solange kleiner $\alpha$ dann gut:\\
     $k/n > p_0 + \delta$ bzw. $k > np_0 + n\delta$\\
     $k/n < p_0 - \delta$ bzw. $k < np_0 - n\delta$
     
* allgemeine Regeln
  - $A, B$ unabhängig => $p(A \cap B) = p(A)*p(B)$ (siehe Seite 14)
  - $n$ Ereignisse heißen $A_1,A_2,...,A_n$ heißen vollständig unabhängig, wenn je $n-1$ dieser\\
    Ereignisse vollständig unabhängig sind und wenn gilt (Siehe seite 16):\\
    $p(A_1 \cap A_2 \cap...\cap A_n) = p(A_1)p(A_2)...p(A_n)$
  - Wie hoch ist die chance das bei $n$ unabhängigen würfen $k$ mal ein Ereignis mit der\\
    wahrscheinlichkeit $p$ Eintritt (siehe Siete 18): ${n \choose k}p^k(1-p)^n-k$
  - Um den gemeinsamen Erwartungswert von mehreren Zufallsvariablen (X, Y, ...) zu ermitteln wenn sie\\
    - abhängig sind: $E(X + Y) = E(X) + E(Y)$
    - unabhängig sind: $E(X*Y) = E(X) * E(Y)$
  - Um die gemeinssame varianz mehrere unabhängiger Zufallsvariablen (X, Y, ...) zu ermitteln:\\
    $Var(X+Y) = Var(X) + Var(Y)$
  - Zentraler Grenzwertsatz Seite 63
    
